{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d87771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from utils.utils import generate_mask, load_model, writeDACFile, sample_top_n\n",
    "from dataloader.dataset import onehot, getNumClasses\n",
    "from utils.utils import interpolate_vectors\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from DACTransformer.DACTransformer import TransformerDecoder\n",
    "from DACTransformer.CondQueryTransformer import ClassConditionedTransformer\n",
    "from DACTransformer.CondKeyTransformer import ClassConditionedKeyTransformer\n",
    "from DACTransformer.PostNormCondDACTransformer import PostNormCondDACTransformerDecoder\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a1f2ab",
   "metadata": {},
   "source": [
    "morphs between infsnd1[i] and infsnd2[i] (and back). If the sounds are the same, then the only thing that interpolates is the paramdeter value Otherwise both the class and the parameter value interpolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### params\n",
    "experiment_name = \"01.20PostNormCond_2_\" \n",
    "checkpoint_dir = 'runs' + '/' + experiment_name\n",
    "cptnum =  600 #params['num_epochs'] # 300 #(must be in the checkpoint directory)\n",
    "\n",
    "# will morph between infsnd1[i] and infsnd2[i] (and back)\n",
    "infsnd1 = ['pistons', 'wind', 'applause', 'bees'] # used for inference\n",
    "#infsnd2 = ['pistons' , 'wind',   'applause', 'bees'] # used for inference\n",
    "infsnd2 = ['applause', 'bees', 'pistons', 'wind' ] # used for inference\n",
    "\n",
    "# Load YAML file\n",
    "with open(checkpoint_dir + '/' + 'params.yaml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "    \n",
    "TransformerClass =  globals().get(params['TransformerClass'])  \n",
    "print(f\"using TransformerClass = {params['TransformerClass']}\") \n",
    "print(f' and TransformerClass is class object {TransformerClass}')\n",
    "\n",
    "cond_size = 8 # num_classes + num params - not a FREE parameter!\n",
    "\n",
    "embed_size = params['tblock_input_size'] -cond_size # 240 #32  # embed_size must be divisible by num_heads and by num tokens\n",
    "print(f'embed_size is {embed_size}')\n",
    "\n",
    "\n",
    "fnamebase='out' + '.e' + str(params['tblock_input_size']-cond_size) + '.l' + str(params['num_layers']) + '.h' + str(params['num_heads']) + '_chkpt_' + str(cptnum).zfill(4) \n",
    "checkpoint_path = checkpoint_dir + '/' +  fnamebase  + '.pth' \n",
    "\n",
    "# for saving sound \n",
    "outdir=checkpoint_dir\n",
    "\n",
    "DEVICE='cpu' #####################################################''cuda'\n",
    "\n",
    "inference_steps=86*20  #86 frames per second\n",
    "\n",
    "\n",
    "# Values for interpolating the parameter value (start at minpval, up to maxpval, and then back)\n",
    "minpval=0\n",
    "maxpval=1\n",
    "topn=1024 # sample from the top n logits\n",
    "\n",
    "\n",
    "print(f'checkpoint_path = {checkpoint_path}, fnamebase = {fnamebase}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e01007",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEVICE == 'cuda' :\n",
    "    torch.cuda.device_count()\n",
    "    torch.cuda.get_device_properties(0).total_memory/1e9\n",
    "\n",
    "    device = torch.device(DEVICE) # if the docker was started with --gpus all, then can choose here with cuda:0 (or cpu)\n",
    "    torch.cuda.device_count()\n",
    "    print(f'memeory on cuda 0 is  {torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
    "else :\n",
    "    device=DEVICE\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5244dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, inference_cond, Ti, vocab_size, num_tokens, inference_steps, fname) :\n",
    "    model.eval()\n",
    "    mask = generate_mask(Ti, Ti).to(device)\n",
    "    input_data = torch.randint(0, vocab_size, (1, Ti, num_tokens)).to(device)  # Smaller context window for inference\n",
    "    predictions = []\n",
    "\n",
    "    t0 = time.time()\n",
    "    for i in range(inference_steps):  # \n",
    "        if cond_size == 0:\n",
    "            output = model(input_data, None, mask) # step through \n",
    "        else : \n",
    "            output = model(input_data, inference_cond[:, i:Ti+i, :], mask) # step through\n",
    "\n",
    "        # This takes the last vector of the sequence (the new predicted token stack) so has size(b,steps,4,1024)\n",
    "        # This it takes the max across the last dimension (scores for each element of the vocabulary (for each of the 4 tokens))\n",
    "        # .max returns a duple of tensors, the first are the max vals (one for each token) and the second are the\n",
    "        #        indices in the range of the vocabulary size. \n",
    "        # THAT IS, the 4 selected \"best\" tokens (one for each codebook) are taken independently\n",
    "        ########################### next_token = output[:, -1, :, :].max(-1)[1]  # Greedy decoding for simplicity\n",
    "        next_token = sample_top_n(output[:, -1, :, :],2) # top 1 would be the same as max in the comment line above\n",
    "            \n",
    "        #print(f'next_token: {next_token} which had a top 1 logit value of {sample_top_n(output[:, -1, :, :],1)}')\n",
    "        #print(f'                                       and a had a top 2 logit value of {sample_top_n(output[:, -1, :, :],2)}')\n",
    "                                                                           \n",
    "                                                                           \n",
    "        \n",
    "        predictions.append(next_token)\n",
    "        input_data = torch.cat([input_data, next_token.unsqueeze(1)], dim=1)[:, 1:]  # Slide window\n",
    "\n",
    "    t1 = time.time()\n",
    "    inf_time = t1-t0\n",
    "    print(f'inference time for {inference_steps} steps, or {inference_steps/86} seconds of sound is {inf_time}' )\n",
    "\n",
    "    dacseq = torch.cat(predictions, dim=0).unsqueeze(0).transpose(1, 2)\n",
    "    if mask == None:\n",
    "        writeDACFile(fname + '_unmasked', dacseq)\n",
    "    else :\n",
    "        writeDACFile(fname, dacseq)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' About to call load_model with TransformerClass = {TransformerClass}')\n",
    "model, Ti, vocab_size, num_codebooks, cond_size = load_model(checkpoint_path,  TransformerClass, DEVICE)\n",
    "\n",
    "# Count the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {num_params}')\n",
    "\n",
    "\n",
    "model.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for snum in range(len(infsnd1)) : \n",
    "    if cond_size == 0 :\n",
    "        inference_cond = None\n",
    "    else : \n",
    "        one_hot_fvector1=onehot(infsnd1[snum]) #The parameter evolution over time will be the same for all sounds\n",
    "        one_hot_fvector2=onehot(infsnd2[snum]) #The parameter evolution over time will be the same for all sounds\n",
    "\n",
    "        cvect1=torch.cat((one_hot_fvector1, torch.tensor([minpval])))\n",
    "        cvect2=torch.cat((one_hot_fvector2, torch.tensor([maxpval])))\n",
    "\n",
    "        print(f'snum = {snum}, cvect2.shape = {cvect2.shape}')\n",
    "\n",
    "        steps=[0,Ti+1*inference_steps//5, Ti+2*inference_steps//5, Ti+3*inference_steps//5, Ti+4*inference_steps//5, Ti+inference_steps]\n",
    "        inference_cond=interpolate_vectors([cvect1,cvect1, cvect2, cvect2, cvect1, cvect1 ], steps) #length must cover staring context window+inf steps\n",
    "\n",
    "        # Extract the 2D array of shape [n, m]\n",
    "        data = inference_cond[0]\n",
    "\n",
    "        # Find components that change over time\n",
    "        changing_indices = [i for i in range(cond_size) if not torch.all(data[:, i] == data[0, i])]\n",
    "\n",
    "        # Format the arrays as strings\n",
    "        cvect1_str = ', '.join(map(str, cvect1.tolist()))\n",
    "        cvect2_str = ', '.join(map(str, cvect2.tolist()))\n",
    "\n",
    "        # Plot the changing components\n",
    "        plt.figure(figsize=(10, 3))\n",
    "\n",
    "        for i in changing_indices:\n",
    "            if i != 7 :\n",
    "                plt.plot(data[:, i], label=f'Component {i}')\n",
    "            else : \n",
    "                plt.plot(data[:, i], label=f'Component {i}', linestyle='--')\n",
    "\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Component Values')\n",
    "        plt.title(f' cvect1 = [{cvect1_str}] \\ncvect2 = [{cvect2_str}]')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        inference_cond=inference_cond.to(device)\n",
    "        print(f'shape of inf_cond is  = {inference_cond.shape}') \n",
    "\n",
    "        \n",
    "        outfname=outdir+\"/\"+ \"dacs\" + \"/\" +  infsnd1[snum] + \".\" + infsnd2[snum] + '_chkpt_' + str(cptnum).zfill(4) +  \"_steps_\"+str(inference_steps).zfill(4)+'.minpval_'+ f\"{minpval:01.2f}\" +'.maxpval_'+ f\"{maxpval:01.2f}\" +'.topn_'+ f\"{topn:04d}\"\n",
    "        print(f'outfname is {outfname}')\n",
    "        inference(model, inference_cond, Ti, vocab_size, num_codebooks, inference_steps, outfname ) \n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be83c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3dee49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b557d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
