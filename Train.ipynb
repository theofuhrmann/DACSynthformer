{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9bb5a36d-0fea-4c17-b799-f7bae8b0db61",
   "metadata": {},
   "source": [
    "### chaptGPT specs   \n",
    "\n",
    "A decoder-only transformer in pytorch to predict 'next output' at each time step. \n",
    "\n",
    "Each time step t is represented by a vector of n=4 tokens from the Descript DAC encoder. \n",
    "The length of the sequence (context window) is Ti=86 for inference, and Tt=8*Ti for training. That is, the context window for training is 8 times the length of the context window for inference. \n",
    "The attention is \"causal\", looking only back in time, and the maximum look-back time for the attention blocks is Ti (even when the sequence is longer during training). That is, the masking matrix is *banded* - triangular to be causal, and limited in lookback which results in a diagonal band). This prevents much of the training on shortened context that happens when tokens are near the beginning of traning examples. \n",
    "\n",
    "The size of the vocabulary (the number of descrete values in each codebook) for each of the n tokens is V=1024. \n",
    "\n",
    "The dataloader will as is usual, supply batches in triplets  (input, target, conditioning info) where the size of each input and output is Tt*n (the sequence length times the number of tokens at each time step). The tokens are indexes for the vocabulary in the range of (0, V-1). The targets are shifted relative to the input sequence by 1 as is typical for networks the learn to predict the output for the next time step. \n",
    "\n",
    "The first layer in the architecture will be a learnable \"multiembedding\" layer that embeds each of the 4 tokens at each time step as an m-dimensional vector. The n m-dimensional vectors are concatenated to provide the n*m dimensional input embeddings for the transformer blocks at each time step. \n",
    "\n",
    "A positional code is is added to the K and Q matricies in each Transformer block using Rotary Position Embedding (RoPE).\n",
    "\n",
    "We use a stack of b transformer blocks that are standard (using layer norms, a relu for activation, and a forward expansion factor of 4 form the linear layer). Each transformer block consumes and produces a context window length sequence of m*n dimensional vectors. \n",
    "\n",
    "After the last transformer block, there is a linear layer that maps the m*n dimensional vectors to the output size which is V*n (the vocabulary size time the number of tokens stacked at each time step). These are the logits that will be fed to the softmax functions (one for each of the n to kens) that provide the probability distribtion across the vocabulary set. We use the criterion nn.CrossEntropyLoss() for computing the loss using the targets provided by the dataloader, and Adam for the optimizer.\n",
    "\n",
    "Again, at inference time, the fixed-length context window is shorter than the training sequence window length, and equal to the maximum look-back time of the attention blocks. The inference process takes the output produced at each time step (a stack of n tokens), and shift them in to a sliding window that is used for input for the next time step. The length of the sequences generated during inference is arbitrary and should be settable with a parameter. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce614c-47d9-4e16-bee8-3fbcc556b08a",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad58d43c-b453-496f-8a43-f0a1722b8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramfile = \"params_combined.yaml\"\n",
    "DEVICE = \"mps\"\n",
    "start_epoch = (\n",
    "    0  # to start from a previous training checkpoint, otherwise must be 0\n",
    ")\n",
    "verboselevel = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9760e14-8c2f-4a14-be94-f1ef300296ce",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d67b59e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from DACTransformer.RopeCondDACTransformer import RopeCondDACTransformer\n",
    "from dataloader.dataset import CustomDACDataset\n",
    "from utils.utils import (\n",
    "    generate_mask,\n",
    "    load_model,\n",
    "    save_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b85ef",
   "metadata": {},
   "source": [
    "### <font color='blue'> Derived parameters </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "638ee684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_size is 64\n",
      "using TransformerClass = RopeCondDACTransformer\n",
      "basefname = out.e64.l4.h8\n",
      "outdir = runs/test_combined\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs/test_combined/params.yaml'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data dir\n",
    "\n",
    "# Load YAML file\n",
    "with open(paramfile, \"r\") as file:\n",
    "    params = yaml.safe_load(file)\n",
    "\n",
    "data_dir = params[\"data_dir\"]\n",
    "data_frames = params[\"data_frames\"]\n",
    "validator_data_dir = params[\"validator_data_dir\"]\n",
    "validator_data_frames = params[\"validator_data_frames\"]\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dataset = CustomDACDataset(\n",
    "    data_dir=data_dir, metadata_excel=data_frames, transforms=None\n",
    ")\n",
    "\n",
    "# ---------     for the transformer  --------------#\n",
    "vocab_size = params[\"vocab_size\"]\n",
    "num_tokens = params[\"num_tokens\"]\n",
    "\n",
    "cond_classes = dataset.get_num_classes()\n",
    "cond_params = params[\"cond_params\"]\n",
    "cond_size = (\n",
    "    cond_classes + cond_params\n",
    ")  # num_classes + num params - not a FREE parameter!\n",
    "\n",
    "# embed_size = params['tblock_input_size'] -cond_size # 240 #32  # embed_size +cond_size must be divisible by num_heads and by num tokens\n",
    "embed_size = params[\n",
    "    \"model_size\"\n",
    "]  # embed_size  must be divisible by num_heads and by num tokens\n",
    "print(f\"embed_size is {embed_size}\")\n",
    "\n",
    "Ti = params[\"Ti\"]\n",
    "Tt = params[\"Tt\"]\n",
    "batch_size = params[\"batch_size\"]\n",
    "\n",
    "sequence_length = Tt  # For training\n",
    "\n",
    "num_layers = params[\"num_layers\"]\n",
    "num_heads = params[\"num_heads\"]\n",
    "forward_expansion = params[\"forward_expansion\"]\n",
    "dropout_rate = params[\"dropout_rate\"]\n",
    "learning_rate = params[\"learning_rate\"]\n",
    "num_epochs = params[\"num_epochs\"]\n",
    "\n",
    "experiment_name = params[\"experiment\"]\n",
    "outdir = \"runs\" + \"/\" + experiment_name\n",
    "basefname = (\n",
    "    \"out\"\n",
    "    + \".e\"\n",
    "    + str(embed_size)\n",
    "    + \".l\"\n",
    "    + str(num_layers)\n",
    "    + \".h\"\n",
    "    + str(num_heads)\n",
    ")\n",
    "\n",
    "ErrorLogRate = params[\"ErrorLogRate\"]  # 10\n",
    "checkpoint_interval = params[\"checkpoint_interval\"]\n",
    "\n",
    "\n",
    "TransformerClass = globals().get(params[\"TransformerClass\"])\n",
    "\n",
    "print(f\"using TransformerClass = {params['TransformerClass']}\")\n",
    "print(f\"basefname = {basefname}\")\n",
    "print(f\"outdir = {outdir}\")\n",
    "\n",
    "###########################################################################\n",
    "# Ensure the destination directory exists\n",
    "# destination_dir = os.path.dirname(outdir + '/' + paramfile)\n",
    "# if not os.path.exists(destination_dir):\n",
    "#    os.makedirs(destination_dir)\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "shutil.copy(\n",
    "    paramfile, outdir + \"/params.yaml\"\n",
    ")  # copy whatever paramfile was used to outdir and name it params.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf3928",
   "metadata": {},
   "source": [
    "### <font color='blue'> Set up cuda. \n",
    "Without it, training runs about 10 times slower  \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ff0adcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.device_count()\n",
    "    torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "    device = torch.device(\n",
    "        DEVICE\n",
    "    )  # if the docker was started with --gpus all, then can choose here with cuda:0 (or cpu)\n",
    "    torch.cuda.device_count()\n",
    "    print(\n",
    "        f\"memeory on cuda 0 is  {torch.cuda.get_device_properties(0).total_memory/1e9}\"\n",
    "    )\n",
    "else:\n",
    "    device = DEVICE\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7020c",
   "metadata": {},
   "source": [
    "### <font color='blue'> Load data \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c684557b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [831, 4] at entry 0 and [430, 4] at entry 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m     validator_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     10\u001b[0m         validator_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Test data dir\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, targets, cvect) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# pass\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Your training code here\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# inputs: batch of input data of shape [batch_size, N, T-1]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# targets: corresponding batch of target data of shape [batch_size, N, T-1]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/School/2/CMC/DACSynthformer/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/School/2/CMC/DACSynthformer/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/School/2/CMC/DACSynthformer/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/School/2/CMC/DACSynthformer/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/School/2/CMC/DACSynthformer/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/School/2/CMC/DACSynthformer/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/School/2/CMC/DACSynthformer/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/School/2/CMC/DACSynthformer/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [831, 4] at entry 0 and [430, 4] at entry 3"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Validator data set\n",
    "if validator_data_dir is not None:\n",
    "    validator_dataset = CustomDACDataset(\n",
    "        data_dir=validator_data_dir, metadata_excel=validator_data_frames\n",
    "    )\n",
    "    validator_dataloader = DataLoader(\n",
    "        validator_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Test data dir\n",
    "for batch_idx, (inputs, targets, cvect) in enumerate(dataloader):\n",
    "    # pass\n",
    "    # Your training code here\n",
    "    # inputs: batch of input data of shape [batch_size, N, T-1]\n",
    "    # targets: corresponding batch of target data of shape [batch_size, N, T-1]\n",
    "\n",
    "    if batch_idx == 0:\n",
    "        print(f\"Batch {batch_idx + 1}\")\n",
    "        print(f\"Inputs shape: {inputs.shape}\")\n",
    "        print(f\"Targets shape: {targets.shape}\")\n",
    "        print(f\"cvect shape: {cvect.shape}\")\n",
    "        print(f\"cevect is {cvect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0eb6c",
   "metadata": {},
   "source": [
    "### <font color='blue'> Make the mask \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3c9b110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask.shape is torch.Size([831, 831])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [-inf, -inf, -inf,  ..., 0., -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., 0., 0., -inf],\n",
       "        [-inf, -inf, -inf,  ..., 0., 0., 0.]], device='mps:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = generate_mask(Tt, Ti).to(device)\n",
    "print(f\"Mask.shape is {mask.shape}\")\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11c4b392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model with embed_size=64, cond_size=3\n",
      " ------------- embed_dim (64) must be divisible by num_heads (8)\n",
      "Setting up MultiEmbedding with vocab_size= 1024, embed_size= 64, num_codebooks= 4\n",
      "Setting up RotaryPositionalEmbedding with embed_size= 64, max_len= 831\n",
      "Total number of parameters: 549120\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model, put it on the device\n",
    "# model = TransformerDecoder(embed_size, num_layers, num_heads, forward_expansion, dropout_rate, Tt, num_tokens, vocab_size).to(device)\n",
    "print(f\"Creating model with embed_size={embed_size}, cond_size={cond_size}\")\n",
    "\n",
    "if start_epoch == 0:\n",
    "    model = TransformerClass(\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        forward_expansion,\n",
    "        dropout_rate,\n",
    "        Tt,\n",
    "        cond_classes,\n",
    "        num_tokens,\n",
    "        vocab_size,\n",
    "        cond_size,\n",
    "        verboselevel,\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    checkpoint_path = (\n",
    "        outdir\n",
    "        + \"/\"\n",
    "        + basefname\n",
    "        + \"_chkpt_\"\n",
    "        + str(start_epoch).zfill(4)\n",
    "        + \".pth\"\n",
    "    )\n",
    "    print(\n",
    "        f\"in train, start_epoch = {start_epoch} and checkpoint_path = {checkpoint_path}\"\n",
    "    )\n",
    "    assert os.path.exists(\n",
    "        checkpoint_path\n",
    "    ), f\"{checkpoint_path} does not exist.\"\n",
    "    if (\n",
    "        start_epoch != 0\n",
    "        and checkpoint_path\n",
    "        and os.path.exists(checkpoint_path)\n",
    "    ):\n",
    "        print(f\"Loading and creating model from {checkpoint_path}\")\n",
    "        # Restore model weights\n",
    "        model, optimizer, _, vocab_size, num_tokens, cond_size = load_model(\n",
    "            checkpoint_path, TransformerClass, device\n",
    "        )\n",
    "        # best_metric = checkpoint['best_metric']  # If you're tracking performance\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Count the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {num_params}\")\n",
    "\n",
    "# Initialize SummaryWriter for tensorboard\n",
    "writer = SummaryWriter(outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85526818-66e9-4bf0-9a94-0334ecd39d61",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2f038",
   "metadata": {},
   "source": [
    "# <font color='blue'> Train !! \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb439d-6d8f-4b1c-a4fd-6acac2a07db6",
   "metadata": {},
   "source": [
    "### loss is average CE across all output tokens\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{n=1}^{N} \\text{CE}(x_n, y_n)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19920362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2  (with max 200), loss: 6.132556915283203\n",
      "Validation Loss: 5.980554580688477\n",
      "train time for 2 epochs, was 2.2240161895751953\n",
      "EPOCH 4  (with max 200), loss: 5.353292465209961\n",
      "Validation Loss: 5.1553826332092285\n",
      "train time for 4 epochs, was 3.3289730548858643\n",
      "EPOCH 6  (with max 200), loss: 4.314689636230469\n",
      "Validation Loss: 4.239997863769531\n",
      "train time for 6 epochs, was 4.398197174072266\n",
      "EPOCH 8  (with max 200), loss: 3.8172976970672607\n",
      "Validation Loss: 3.8209729194641113\n",
      "train time for 8 epochs, was 5.462704181671143\n",
      "EPOCH 10  (with max 200), loss: 3.6688406467437744\n",
      "Validation Loss: 3.780838966369629\n",
      "train time for 10 epochs, was 6.5399250984191895\n",
      "EPOCH 12  (with max 200), loss: 3.6147379875183105\n",
      "Validation Loss: 3.724679946899414\n",
      "train time for 12 epochs, was 7.803961277008057\n",
      "EPOCH 14  (with max 200), loss: 3.5895702838897705\n",
      "Validation Loss: 3.732815742492676\n",
      "train time for 14 epochs, was 8.98399305343628\n",
      "EPOCH 16  (with max 200), loss: 3.524400234222412\n",
      "Validation Loss: 3.71219539642334\n",
      "train time for 16 epochs, was 10.102882146835327\n",
      "EPOCH 18  (with max 200), loss: 3.611111879348755\n",
      "Validation Loss: 3.7029805183410645\n",
      "train time for 18 epochs, was 11.158938884735107\n",
      "EPOCH 20  (with max 200), loss: 3.580322504043579\n",
      "Validation Loss: 3.65063214302063\n",
      "train time for 20 epochs, was 12.214855909347534\n",
      "EPOCH 22  (with max 200), loss: 3.5913047790527344\n",
      "Validation Loss: 3.625814437866211\n",
      "train time for 22 epochs, was 13.387304067611694\n",
      "EPOCH 24  (with max 200), loss: 3.57851243019104\n",
      "Validation Loss: 3.681190013885498\n",
      "train time for 24 epochs, was 14.566103219985962\n",
      "EPOCH 26  (with max 200), loss: 3.438519239425659\n",
      "Validation Loss: 3.6763341426849365\n",
      "train time for 26 epochs, was 15.614561080932617\n",
      "EPOCH 28  (with max 200), loss: 3.3618805408477783\n",
      "Validation Loss: 3.6030349731445312\n",
      "train time for 28 epochs, was 16.667789220809937\n",
      "EPOCH 30  (with max 200), loss: 3.4436891078948975\n",
      "Validation Loss: 3.671349048614502\n",
      "train time for 30 epochs, was 17.727436065673828\n",
      "EPOCH 32  (with max 200), loss: 3.2688324451446533\n",
      "Validation Loss: 3.673511266708374\n",
      "train time for 32 epochs, was 18.7866530418396\n",
      "EPOCH 34  (with max 200), loss: 3.526210069656372\n",
      "Validation Loss: 3.6692681312561035\n",
      "train time for 34 epochs, was 19.840480089187622\n",
      "EPOCH 36  (with max 200), loss: 3.3113045692443848\n",
      "Validation Loss: 3.665760040283203\n",
      "train time for 36 epochs, was 20.893765211105347\n",
      "EPOCH 38  (with max 200), loss: 3.308319568634033\n",
      "Validation Loss: 3.673567533493042\n",
      "train time for 38 epochs, was 21.963252067565918\n",
      "EPOCH 40  (with max 200), loss: 3.3976023197174072\n",
      "Validation Loss: 3.667985439300537\n",
      "train time for 40 epochs, was 23.017158031463623\n",
      "EPOCH 42  (with max 200), loss: 3.2473440170288086\n",
      "Validation Loss: 3.6369986534118652\n",
      "train time for 42 epochs, was 24.063465118408203\n",
      "EPOCH 44  (with max 200), loss: 3.281635046005249\n",
      "Validation Loss: 3.6660804748535156\n",
      "train time for 44 epochs, was 25.102424144744873\n",
      "EPOCH 46  (with max 200), loss: 3.2248141765594482\n",
      "Validation Loss: 3.5971808433532715\n",
      "train time for 46 epochs, was 26.14181113243103\n",
      "EPOCH 48  (with max 200), loss: 3.1540420055389404\n",
      "Validation Loss: 3.6636319160461426\n",
      "train time for 48 epochs, was 27.18324899673462\n",
      "EPOCH 50  (with max 200), loss: 3.1515469551086426\n",
      "Validation Loss: 3.5952110290527344\n",
      "train time for 50 epochs, was 28.294047117233276\n",
      "EPOCH 50 save model to : runs/test_new_2/out.e64.l4.h8_chkpt_0050.pth\n",
      "EPOCH 52  (with max 200), loss: 3.0614283084869385\n",
      "Validation Loss: 3.561591148376465\n",
      "train time for 52 epochs, was 29.571171045303345\n",
      "EPOCH 54  (with max 200), loss: 3.250324249267578\n",
      "Validation Loss: 3.6283257007598877\n",
      "train time for 54 epochs, was 30.63219404220581\n",
      "EPOCH 56  (with max 200), loss: 3.1536049842834473\n",
      "Validation Loss: 3.6655285358428955\n",
      "train time for 56 epochs, was 31.698354959487915\n",
      "EPOCH 58  (with max 200), loss: 3.0053915977478027\n",
      "Validation Loss: 3.661794424057007\n",
      "train time for 58 epochs, was 32.75459098815918\n",
      "EPOCH 60  (with max 200), loss: 2.964481830596924\n",
      "Validation Loss: 3.6343140602111816\n",
      "train time for 60 epochs, was 33.81611490249634\n",
      "EPOCH 62  (with max 200), loss: 3.3087875843048096\n",
      "Validation Loss: 3.594015121459961\n",
      "train time for 62 epochs, was 34.89666414260864\n",
      "EPOCH 64  (with max 200), loss: 3.266110420227051\n",
      "Validation Loss: 3.5960724353790283\n",
      "train time for 64 epochs, was 35.96607518196106\n",
      "EPOCH 66  (with max 200), loss: 3.0374770164489746\n",
      "Validation Loss: 3.6461129188537598\n",
      "train time for 66 epochs, was 37.03044819831848\n",
      "EPOCH 68  (with max 200), loss: 3.114302158355713\n",
      "Validation Loss: 3.6024208068847656\n",
      "train time for 68 epochs, was 38.090728998184204\n",
      "EPOCH 70  (with max 200), loss: 3.2750368118286133\n",
      "Validation Loss: 3.6814632415771484\n",
      "train time for 70 epochs, was 39.16126489639282\n",
      "EPOCH 72  (with max 200), loss: 3.0789506435394287\n",
      "Validation Loss: 3.6106386184692383\n",
      "train time for 72 epochs, was 40.23205614089966\n",
      "EPOCH 74  (with max 200), loss: 3.01113224029541\n",
      "Validation Loss: 3.665381669998169\n",
      "train time for 74 epochs, was 41.29658508300781\n",
      "EPOCH 76  (with max 200), loss: 2.9361140727996826\n",
      "Validation Loss: 3.5897068977355957\n",
      "train time for 76 epochs, was 42.34664297103882\n",
      "EPOCH 78  (with max 200), loss: 3.1566224098205566\n",
      "Validation Loss: 3.713106155395508\n",
      "train time for 78 epochs, was 43.40140223503113\n",
      "EPOCH 80  (with max 200), loss: 2.940971851348877\n",
      "Validation Loss: 3.709031343460083\n",
      "train time for 80 epochs, was 44.46345806121826\n",
      "EPOCH 82  (with max 200), loss: 2.8643112182617188\n",
      "Validation Loss: 3.719538688659668\n",
      "train time for 82 epochs, was 45.52325987815857\n",
      "EPOCH 84  (with max 200), loss: 2.833242893218994\n",
      "Validation Loss: 3.7023239135742188\n",
      "train time for 84 epochs, was 46.57877516746521\n",
      "EPOCH 86  (with max 200), loss: 2.9284660816192627\n",
      "Validation Loss: 3.7424588203430176\n",
      "train time for 86 epochs, was 47.64128303527832\n",
      "EPOCH 88  (with max 200), loss: 3.001495838165283\n",
      "Validation Loss: 3.755241870880127\n",
      "train time for 88 epochs, was 48.693061113357544\n",
      "EPOCH 90  (with max 200), loss: 3.1908040046691895\n",
      "Validation Loss: 3.6747913360595703\n",
      "train time for 90 epochs, was 49.747262954711914\n",
      "EPOCH 92  (with max 200), loss: 2.736978769302368\n",
      "Validation Loss: 3.738495349884033\n",
      "train time for 92 epochs, was 50.802826166152954\n",
      "EPOCH 94  (with max 200), loss: 2.9354586601257324\n",
      "Validation Loss: 3.7891364097595215\n",
      "train time for 94 epochs, was 51.859193086624146\n",
      "EPOCH 96  (with max 200), loss: 3.0161402225494385\n",
      "Validation Loss: 3.7676730155944824\n",
      "train time for 96 epochs, was 52.91398310661316\n",
      "EPOCH 98  (with max 200), loss: 2.7336151599884033\n",
      "Validation Loss: 3.7161285877227783\n",
      "train time for 98 epochs, was 53.97268319129944\n",
      "EPOCH 100  (with max 200), loss: 2.8296375274658203\n",
      "Validation Loss: 3.8058037757873535\n",
      "train time for 100 epochs, was 55.029229164123535\n",
      "EPOCH 100 save model to : runs/test_new_2/out.e64.l4.h8_chkpt_0100.pth\n",
      "EPOCH 102  (with max 200), loss: 2.881960868835449\n",
      "Validation Loss: 3.6945250034332275\n",
      "train time for 102 epochs, was 56.13491106033325\n",
      "EPOCH 104  (with max 200), loss: 2.719072103500366\n",
      "Validation Loss: 3.836529016494751\n",
      "train time for 104 epochs, was 57.19458317756653\n",
      "EPOCH 106  (with max 200), loss: 2.8570423126220703\n",
      "Validation Loss: 3.7695281505584717\n",
      "train time for 106 epochs, was 58.243921995162964\n",
      "EPOCH 108  (with max 200), loss: 2.712205171585083\n",
      "Validation Loss: 3.73481822013855\n",
      "train time for 108 epochs, was 59.2955379486084\n",
      "EPOCH 110  (with max 200), loss: 2.7168033123016357\n",
      "Validation Loss: 3.7971272468566895\n",
      "train time for 110 epochs, was 60.35235619544983\n",
      "EPOCH 112  (with max 200), loss: 2.6780340671539307\n",
      "Validation Loss: 3.759406089782715\n",
      "train time for 112 epochs, was 61.40661001205444\n",
      "EPOCH 114  (with max 200), loss: 2.874420166015625\n",
      "Validation Loss: 3.7781898975372314\n",
      "train time for 114 epochs, was 62.45838022232056\n",
      "EPOCH 116  (with max 200), loss: 2.8223955631256104\n",
      "Validation Loss: 3.8932721614837646\n",
      "train time for 116 epochs, was 63.50698709487915\n",
      "EPOCH 118  (with max 200), loss: 2.8344435691833496\n",
      "Validation Loss: 3.902388095855713\n",
      "train time for 118 epochs, was 64.56427717208862\n",
      "EPOCH 120  (with max 200), loss: 2.7839787006378174\n",
      "Validation Loss: 3.814948558807373\n",
      "train time for 120 epochs, was 65.60852003097534\n",
      "EPOCH 122  (with max 200), loss: 2.700374126434326\n",
      "Validation Loss: 3.9601023197174072\n",
      "train time for 122 epochs, was 66.66776704788208\n",
      "EPOCH 124  (with max 200), loss: 2.769430160522461\n",
      "Validation Loss: 3.9928436279296875\n",
      "train time for 124 epochs, was 67.73126816749573\n",
      "EPOCH 126  (with max 200), loss: 2.759962558746338\n",
      "Validation Loss: 4.009659767150879\n",
      "train time for 126 epochs, was 68.7849280834198\n",
      "EPOCH 128  (with max 200), loss: 2.6725480556488037\n",
      "Validation Loss: 4.018132209777832\n",
      "train time for 128 epochs, was 69.83499813079834\n",
      "EPOCH 130  (with max 200), loss: 2.650851249694824\n",
      "Validation Loss: 4.037353992462158\n",
      "train time for 130 epochs, was 70.8934211730957\n",
      "EPOCH 132  (with max 200), loss: 2.4979474544525146\n",
      "Validation Loss: 3.9764885902404785\n",
      "train time for 132 epochs, was 71.95051908493042\n",
      "EPOCH 134  (with max 200), loss: 2.7898552417755127\n",
      "Validation Loss: 4.081202983856201\n",
      "train time for 134 epochs, was 73.00159215927124\n",
      "EPOCH 136  (with max 200), loss: 2.636760950088501\n",
      "Validation Loss: 3.947556257247925\n",
      "train time for 136 epochs, was 74.05965900421143\n",
      "EPOCH 138  (with max 200), loss: 2.606194496154785\n",
      "Validation Loss: 4.1194539070129395\n",
      "train time for 138 epochs, was 75.11295413970947\n",
      "EPOCH 140  (with max 200), loss: 2.5826404094696045\n",
      "Validation Loss: 3.9784557819366455\n",
      "train time for 140 epochs, was 76.16237592697144\n",
      "EPOCH 142  (with max 200), loss: 2.6092493534088135\n",
      "Validation Loss: 4.071652412414551\n",
      "train time for 142 epochs, was 77.21738123893738\n",
      "EPOCH 144  (with max 200), loss: 2.442847490310669\n",
      "Validation Loss: 4.132359504699707\n",
      "train time for 144 epochs, was 78.26170015335083\n",
      "EPOCH 146  (with max 200), loss: 2.6033449172973633\n",
      "Validation Loss: 4.032321929931641\n",
      "train time for 146 epochs, was 79.31089806556702\n",
      "EPOCH 148  (with max 200), loss: 2.442546844482422\n",
      "Validation Loss: 4.211750030517578\n",
      "train time for 148 epochs, was 80.36009812355042\n",
      "EPOCH 150  (with max 200), loss: 2.472848892211914\n",
      "Validation Loss: 4.143654823303223\n",
      "train time for 150 epochs, was 81.41036796569824\n",
      "EPOCH 150 save model to : runs/test_new_2/out.e64.l4.h8_chkpt_0150.pth\n",
      "EPOCH 152  (with max 200), loss: 2.4049971103668213\n",
      "Validation Loss: 4.087767124176025\n",
      "train time for 152 epochs, was 82.51564407348633\n",
      "EPOCH 154  (with max 200), loss: 2.4897501468658447\n",
      "Validation Loss: 4.226410865783691\n",
      "train time for 154 epochs, was 83.55805897712708\n",
      "EPOCH 156  (with max 200), loss: 2.501373767852783\n",
      "Validation Loss: 4.127328395843506\n",
      "train time for 156 epochs, was 84.60235118865967\n",
      "EPOCH 158  (with max 200), loss: 2.48418927192688\n",
      "Validation Loss: 4.232024192810059\n",
      "train time for 158 epochs, was 85.65802717208862\n",
      "EPOCH 160  (with max 200), loss: 2.541376829147339\n",
      "Validation Loss: 4.156952857971191\n",
      "train time for 160 epochs, was 86.7120909690857\n",
      "EPOCH 162  (with max 200), loss: 2.459841012954712\n",
      "Validation Loss: 4.333952903747559\n",
      "train time for 162 epochs, was 87.76276898384094\n",
      "EPOCH 164  (with max 200), loss: 2.4907636642456055\n",
      "Validation Loss: 4.358800888061523\n",
      "train time for 164 epochs, was 88.8107259273529\n",
      "EPOCH 166  (with max 200), loss: 2.3810510635375977\n",
      "Validation Loss: 4.378200054168701\n",
      "train time for 166 epochs, was 89.86609506607056\n",
      "EPOCH 168  (with max 200), loss: 2.3726580142974854\n",
      "Validation Loss: 4.4225568771362305\n",
      "train time for 168 epochs, was 90.91252112388611\n",
      "EPOCH 170  (with max 200), loss: 2.277491569519043\n",
      "Validation Loss: 4.332221031188965\n",
      "train time for 170 epochs, was 91.96499013900757\n",
      "EPOCH 172  (with max 200), loss: 2.3127858638763428\n",
      "Validation Loss: 4.34438419342041\n",
      "train time for 172 epochs, was 93.01601910591125\n",
      "EPOCH 174  (with max 200), loss: 2.340485095977783\n",
      "Validation Loss: 4.458869457244873\n",
      "train time for 174 epochs, was 94.06118416786194\n",
      "EPOCH 176  (with max 200), loss: 2.1544251441955566\n",
      "Validation Loss: 4.511994361877441\n",
      "train time for 176 epochs, was 95.10859107971191\n",
      "EPOCH 178  (with max 200), loss: 2.3348886966705322\n",
      "Validation Loss: 4.487672805786133\n",
      "train time for 178 epochs, was 96.15761709213257\n",
      "EPOCH 180  (with max 200), loss: 2.2689707279205322\n",
      "Validation Loss: 4.48638391494751\n",
      "train time for 180 epochs, was 97.25587797164917\n",
      "EPOCH 182  (with max 200), loss: 2.2606148719787598\n",
      "Validation Loss: 4.357080459594727\n",
      "train time for 182 epochs, was 98.33764314651489\n",
      "EPOCH 184  (with max 200), loss: 2.277240037918091\n",
      "Validation Loss: 4.5126872062683105\n",
      "train time for 184 epochs, was 99.3738660812378\n",
      "EPOCH 186  (with max 200), loss: 2.118607997894287\n",
      "Validation Loss: 4.5373640060424805\n",
      "train time for 186 epochs, was 100.41084003448486\n",
      "EPOCH 188  (with max 200), loss: 2.367619276046753\n",
      "Validation Loss: 4.511622905731201\n",
      "train time for 188 epochs, was 101.44823002815247\n",
      "EPOCH 190  (with max 200), loss: 2.1242740154266357\n",
      "Validation Loss: 4.434288024902344\n",
      "train time for 190 epochs, was 102.48745703697205\n",
      "EPOCH 192  (with max 200), loss: 2.1788411140441895\n",
      "Validation Loss: 4.597711563110352\n",
      "train time for 192 epochs, was 103.52606415748596\n",
      "EPOCH 194  (with max 200), loss: 2.269341468811035\n",
      "Validation Loss: 4.667118072509766\n",
      "train time for 194 epochs, was 104.56673622131348\n",
      "EPOCH 196  (with max 200), loss: 2.141611099243164\n",
      "Validation Loss: 4.477662563323975\n",
      "train time for 196 epochs, was 105.60433220863342\n",
      "EPOCH 198  (with max 200), loss: 2.190948724746704\n",
      "Validation Loss: 4.71186637878418\n",
      "train time for 198 epochs, was 106.64593625068665\n",
      "EPOCH 200  (with max 200), loss: 2.1347577571868896\n",
      "Validation Loss: 4.518854141235352\n",
      "train time for 200 epochs, was 107.68491697311401\n",
      "EPOCH 200 save model to : runs/test_new_2/out.e64.l4.h8_chkpt_0200.pth\n",
      "train time for 200 epochs, was 107.73514604568481\n",
      "loss  =  2.1347577571868896\n"
     ]
    }
   ],
   "source": [
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    dataloader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    outdir,\n",
    "    basefname,\n",
    "    start_epoch=0,\n",
    "    checkpoint_path=None,\n",
    "):\n",
    "    t0 = time.time()\n",
    "    max_epoch = start_epoch + num_epochs\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        for batch_idx, (input_data, target_data, cond_data) in enumerate(\n",
    "            dataloader\n",
    "        ):\n",
    "            if verboselevel > 5:\n",
    "                print(\n",
    "                    f\" ---- submitting batch with input_data={input_data.shape}, target_data={target_data.shape}, cond_data={cond_data.shape}\"\n",
    "                )\n",
    "            # print(f\"b{batch_idx} \", end='')\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move inputs and targets to the device\n",
    "            input_data, target_data, cond_data = (\n",
    "                input_data.to(device),\n",
    "                target_data.to(device),\n",
    "                cond_data.to(device),\n",
    "            )\n",
    "\n",
    "            if cond_size == 0:  # Ignore conditioning data\n",
    "                cond_expanded = None\n",
    "            else:\n",
    "                # for dataset exammples, expand the conditioning info across all time steps before passing to models\n",
    "                cond_expanded = cond_data.unsqueeze(1).expand(\n",
    "                    -1, input_data.size(1), -1\n",
    "                )\n",
    "\n",
    "            \"\"\"print(\n",
    "                f\"    after loading a batch,  input_data.shape is {input_data.shape}, and cond_data.shape is {cond_data.shape}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"    after loading a batch,  cond_expanded.shape is {cond_expanded.shape}\"\n",
    "            )\n",
    "            print(f\"    after loading a batch,  mask.shape is {mask.shape}\")\n",
    "            print(f\" model={model}\")\"\"\"\n",
    "\n",
    "            # torch.Size([batch_size, seq_len, num_tokens, vocab_size])\n",
    "            output = model(input_data, cond_expanded, mask)\n",
    "\n",
    "            if verboselevel > 5:\n",
    "                print(\n",
    "                    f\" TTTTTTTT after training, output shape ={output.shape}, torch.Size([batch_size, seq_len, num_tokens, vocab_size])\"\n",
    "                )\n",
    "                print(\n",
    "                    f\" TTTTTTTT Passing to CRITERION with , output.reshape(-1, vocab_size) = {output.reshape(-1, vocab_size).shape} and target_data.reshape(-1) = {target_data.reshape(-1).shape}\"\n",
    "                )\n",
    "\n",
    "            ##  this works, but is too verbose >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            ##      # Original shape: (batch_size, seq_len, num_tokens, vocab_size)\n",
    "            ##      output = output.reshape(batch_size, sequence_length * num_tokens, vocab_size)\n",
    "            ##      # Original shape: (batch_size, seq_len, num_tokens)\n",
    "            ##      targets = targets.reshape(batch_size, sequence_length * num_tokens)\n",
    "            ##      loss = criterion(output.permute(0, 2, 1), targets)\n",
    "\n",
    "            ##  more succinct <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "            #   Computes the CE for each token separately, and then averages them to get the loss.\n",
    "            # loss = criterion(output.reshape(-1, vocab_size), target_data.reshape(-1)) # collapses all target_data dimensions into a single dimension\n",
    "            loss = criterion(\n",
    "                output.reshape(-1, vocab_size), target_data.reshape(-1).long()\n",
    "            )\n",
    "            ## <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch + 1) % ErrorLogRate == 0:\n",
    "            print(f\"EPOCH {epoch+1}  (with max {max_epoch}), \", end=\"\")\n",
    "            print(f\"loss: {loss}\")\n",
    "            # Log the loss to TensorBoard\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "            if validator_data_dir is not None:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0\n",
    "                    for (\n",
    "                        val_inputs,\n",
    "                        val_targets,\n",
    "                        cond_data,\n",
    "                    ) in validator_dataloader:\n",
    "                        val_inputs, val_targets, cond_data = (\n",
    "                            val_inputs.to(device),\n",
    "                            val_targets.to(device),\n",
    "                            cond_data.to(device),\n",
    "                        )\n",
    "\n",
    "                        if cond_size == 0:  # Ignore conditioning data\n",
    "                            cond_expanded = None\n",
    "                        else:\n",
    "                            # for dataset exammples, expand the conditioning info across all time steps before passing to models\n",
    "                            cond_expanded = cond_data.unsqueeze(1).expand(\n",
    "                                -1, input_data.size(1), -1\n",
    "                            )\n",
    "\n",
    "                        val_outputs = model(val_inputs, cond_expanded, mask)\n",
    "\n",
    "                        val_loss += criterion(\n",
    "                            val_outputs.reshape(-1, vocab_size),\n",
    "                            val_targets.reshape(-1).long(),\n",
    "                        )  # collapses all target_data dimensions into a single dimension\n",
    "                        # val_loss += criterion(val_outputs, val_targets).item()\n",
    "\n",
    "                print(\n",
    "                    f\"Validation Loss: {val_loss / len(validator_dataloader)}\"\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"Loss/validation\",\n",
    "                    val_loss / len(validator_dataloader),\n",
    "                    epoch,\n",
    "                )\n",
    "\n",
    "                t1 = time.time()\n",
    "                train_time = t1 - t0\n",
    "                print(\n",
    "                    f\"train time for {epoch-start_epoch+1} epochs, was {train_time}\"\n",
    "                )\n",
    "\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            lastbasename = (\n",
    "                outdir + \"/\" + basefname + \"_chkpt_\" + str(epoch + 1).zfill(4)\n",
    "            )\n",
    "            print(f\"EPOCH {epoch+1} save model to : {lastbasename}.pth\")\n",
    "            save_model(model, optimizer, Ti, lastbasename + \".pth\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    train_time = t1 - t0\n",
    "    print(f\"train time for {num_epochs} epochs, was {train_time}\")\n",
    "    print(f\"loss  =  {loss}\")\n",
    "\n",
    "\n",
    "## -----------------------------------------------------------------------------------\n",
    "## OK, let's do it!\n",
    "train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    dataloader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    outdir,\n",
    "    basefname,\n",
    "    start_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16eb43b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just check that inference attention mask will look right\n",
    "# Actually, the inference mask can be None since we are using a context window only as long as the maximum look-back in the training mask\n",
    "# thats why taking the mask with :TI is upper-triangular. Longer dims would show a banded mask again.\n",
    "foo = mask[:Ti, :Ti]\n",
    "foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a1f25-78a6-479b-8771-2910cf639d67",
   "metadata": {},
   "source": [
    "### <font color='blue'> Use Inference.Decode.ipynb to see and hear your generated audio   \n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
