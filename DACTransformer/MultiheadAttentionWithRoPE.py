import torch.nn as nn
import torch.nn.functional as F


class MultiheadAttentionWithRoPE(nn.MultiheadAttention):
    def __init__(
        self,
        embed_dim,
        num_heads,
        rotary_positional_embedding,
        verbose=0,
        **kwargs,
    ):
        super().__init__(embed_dim, num_heads, **kwargs)
        self.rotary_positional_embedding = rotary_positional_embedding
        self.verbose = verbose
        if verbose > 0:
            print(
                f"Initializing MultiheadAttentionWithRoPE with embed_dim={embed_dim}, and num_heads={num_heads}"
            )

    def forward(self, query, key, value, **kwargs):
        if self.verbose > 0:
            print(
                f"MultiheadAttentionWithRoPE with query.shape={query.shape}, key.shape={key.shape}, value.shape={value.shape}"
            )

        # Manually compute Q, K, V projections
        q = F.linear(
            query,
            self.in_proj_weight[: self.embed_dim],
            self.in_proj_bias[: self.embed_dim],
        )
        k = F.linear(
            key,
            self.in_proj_weight[self.embed_dim : 2 * self.embed_dim],
            self.in_proj_bias[self.embed_dim : 2 * self.embed_dim],
        )
        v = F.linear(
            value,
            self.in_proj_weight[2 * self.embed_dim :],
            self.in_proj_bias[2 * self.embed_dim :],
        )

        if self.verbose > 0:
            print(
                f"MultiheadAttentionWithRoPE with q.shape={q.shape}, k.shape={k.shape}, v.shape={v.shape}"
            )

        # Apply Rotary Positional Encoding (RoPE) to Q and K
        q = self.rotary_positional_embedding(q)
        k = self.rotary_positional_embedding(k)

        if self.verbose > 0:
            print(f"After rotary coding, q.shape={q.shape}, k.shape={k.shape}")

        # Use the parent's forward method for attention computation
        return super().forward(q, k, v, **kwargs)
