{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45a40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from utils.utils import generate_mask, load_model, writeDACFile, sample_top_n\n",
    "from dataloader.dataset import onehot, getNumClasses, class_name_to_int, int2classname\n",
    "from utils.utils import interpolate_vectors\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from DACTransformer.DACTransformer import TransformerDecoder\n",
    "from DACTransformer.CondQueryTransformer import ClassConditionedTransformer\n",
    "from DACTransformer.CondKeyTransformer import ClassConditionedKeyTransformer\n",
    "from DACTransformer.PostNormCondDACTransformer import PostNormCondDACTransformerDecoder\n",
    "from DACTransformer.RopeCondDACTransformer import RopeCondDACTransformer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dac\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37d063-8428-4c63-b6e4-a7eac31283cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### params ##########################################################\n",
    "\n",
    "experiment_name=\"2025.02.08_wereback\" #\"smalltest\" #\"02.01.new.rope_bigmodel\" #'01.27.new.rope'\n",
    "checkpoint_dir = 'runs' + '/' + experiment_name\n",
    "paramfile = checkpoint_dir + '/' +  'params.yaml' \n",
    "\n",
    "cptnum =  800 # (must be in the checkpoint directory)\n",
    "SAVEWAV=True\n",
    "DEVICE='cpu' #######''cuda'\n",
    "gendur=20\n",
    "topn=10 # sample from the top n logits\n",
    "\n",
    "###########################################################################\n",
    "#  Choose a breakpoint sequence (and/or make one yourself) ...\n",
    "###########################################################################\n",
    "morphname='applause.param.range'  ###   (choose from breakpoint sets defined below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bcbcd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For your reference:\n",
    "classes=['pistons', 'wind', 'applause', 'bees', 'chirps', 'toks', 'peeps']\n",
    "print(f' ------- One hot vectors for classes ----------')\n",
    "for i in range(len(classes)):\n",
    "    print(f' {classes[i]} : \\t{onehot(classes[i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05903b1",
   "metadata": {},
   "source": [
    "Morph over a vectors in vsequence lineary for (noramlized) time steps vtimes. Create your sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1085a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########################################################################\n",
    "#  These first four just explore the parameter range for each class used for training ...\n",
    "###########################################################################\n",
    "\n",
    "morphs={}\n",
    "morphs['pistons.param.range']= {\n",
    "    'vsequence' : [\n",
    "        torch.tensor([1., 0., 0., 0., 0., 0., 0., 0.0]),\n",
    "        torch.tensor([1., 0., 0., 0., 0., 0., 0., 0.0]),\n",
    "        torch.tensor([1., 0., 0., 0., 0., 0., 0., 1.0]), \n",
    "        torch.tensor([1., 0., 0., 0., 0., 0., 0., 1.0]),\n",
    "        torch.tensor([1., 0., 0., 0., 0., 0., 0., 0.0]),\n",
    "        torch.tensor([1., 0., 0., 0., 0., 0., 0., 0.0]),\n",
    "    ],\n",
    "    'vtimes' : [0,.1, .4, .6, .9, 1] # must be the same length as the number of break points in vsequence\n",
    "}\n",
    "\n",
    "\n",
    "morphs['wind.param.range']= {    \n",
    "    'vsequence': [\n",
    "        torch.tensor([0., 1., 0., 0., 0., 0., 0., 0.0]),\n",
    "        torch.tensor([0., 1., 0., 0., 0., 0., 0., 0.0]),\n",
    "        torch.tensor([0., 1., 0., 0., 0., 0., 0., 1.0]), \n",
    "        torch.tensor([0., 1., 0., 0., 0., 0., 0., 1.0]),\n",
    "        torch.tensor([0., 1., 0., 0., 0., 0., 0., 0.0]),\n",
    "        torch.tensor([0., 1., 0., 0., 0., 0., 0., 0.0]),\n",
    "    ],\n",
    "    'vtimes': [0,.1, .4, .6, .9, 1] # must be the same length as the number of break points in vsequence\n",
    "}\n",
    "\n",
    "morphs['applause.param.range'] = {\n",
    "    'vsequence': [\n",
    "        torch.tensor([0., 0., 1., 0., 0., 0., 0., 0.0]),\n",
    "        torch.tensor([0., 0., 1., 0., 0., 0., 0., 0.0]),\n",
    "        torch.tensor([0., 0., 1., 0., 0., 0., 0., 1.0]), \n",
    "        torch.tensor([0., 0., 1., 0., 0., 0., 0., 1.0]),\n",
    "        torch.tensor([0., 0., 1., 0., 0., 0., 0., 0.0]),\n",
    "        torch.tensor([0., 0., 1., 0., 0., 0., 0., 0.0]),\n",
    "    ],\n",
    "    'vtimes': [0,.1, .4, .6, .9, 1] # must be the same length as the number of break points in vsequence\n",
    "}\n",
    "\n",
    "morphs['bees.param.range'] = {\n",
    "    'vsequence': [\n",
    "        torch.tensor([0., 0., 0., 1., 0., 0., 0., 0.0]),\n",
    "        torch.tensor([0., 0., 0., 1., 0., 0., 0., 0.0]),\n",
    "        torch.tensor([0., 0., 0., 1., 0., 0., 0., 1.0]), \n",
    "        torch.tensor([0., 0., 0., 1., 0., 0., 0., 1.0]),\n",
    "        torch.tensor([0., 0., 0., 1., 0., 0., 0., 0.0]),\n",
    "        torch.tensor([0., 0., 0., 1., 0., 0., 0., 0.0]),\n",
    "    ],\n",
    "    'vtimes' : [0,.1, .4, .6, .9, 1] # must be the same length as the number of break points in vsequence\n",
    "}\n",
    "\n",
    "morphs['chirps.param.range'] = {\n",
    "    'vsequence': [\n",
    "        torch.tensor([0., 0., 0., 0., 1., 0., 0., 0.0]),\n",
    "        torch.tensor([0., 0., 0., 0., 1., 0., 0., 0.0]),\n",
    "        torch.tensor([0., 0., 0., 0., 1., 0., 0., 1.0]), \n",
    "        torch.tensor([0., 0., 0., 0., 1., 0., 0., 1.0]),\n",
    "        torch.tensor([0., 0., 0., 0., 1., 0., 0., 0.0]),\n",
    "        torch.tensor([0., 0., 0., 0., 1., 0., 0., 0.0]),\n",
    "    ],\n",
    "    'vtimes': [0,.1, .4, .6, .9, 1] # must be the same length as the number of break points in vsequence\n",
    "}\n",
    "\n",
    "morphs['toks.param.range'] = {\n",
    "    'vsequence': [\n",
    "        torch.tensor([0., 0., 0., 0., 0., 1., 0., 0.0]),\n",
    "        torch.tensor([0., 0., 0., 0., 0., 1., 0., 0.0]),\n",
    "        torch.tensor([0., 0., 0., 0., 0., 1., 0., 1.0]), \n",
    "        torch.tensor([0., 0., 0., 0., 0., 1., 0., 1.0]),\n",
    "        torch.tensor([0., 0., 0., 0., 0., 1., 0., 0.0]),\n",
    "        torch.tensor([0., 0., 0., 0., 0., 1., 0., 0.0]),\n",
    "    ],\n",
    "    'vtimes': [0,.1, .4, .6, .9, 1] # must be the same length as the number of break points in vsequence\n",
    "}\n",
    "\n",
    "morphs['peeps.param.range'] = {\n",
    "    'vsequence': [\n",
    "        torch.tensor([0., 0., 0., 0., 0., 0., 1., 0.0]),\n",
    "        torch.tensor([0., 0., 0., 0., 0., 0., 1., 0.0]),\n",
    "        torch.tensor([0., 0., 0., 0., 0., 0., 1., 1.0]), \n",
    "        torch.tensor([0., 0., 0., 0., 0., 0., 1., 1.0]),\n",
    "        torch.tensor([0., 0., 0., 0., 0., 0., 1., 0.0]),\n",
    "        torch.tensor([0., 0., 0., 0., 0., 0., 1., 0.0]),\n",
    "    ],\n",
    "    'vtimes': [0,.1, .4, .6, .9, 1] # must be the same length as the number of break points in vsequence\n",
    "}\n",
    "\n",
    "###########################################################################\n",
    "#  Can we suddnely switch between any class? \n",
    "###########################################################################\n",
    "\n",
    "morphs['conditioning response'] = {\n",
    "    # responsiveness to conditioning\n",
    "    'vsequence': [\n",
    "        torch.tensor([1., 0., 0., 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([1., 0., 0., 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0., 1., 0., 0., 0., 0., 0., 0.5]), \n",
    "        torch.tensor([0., 1., 0., 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0., 0., 1., 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0., 0., 1., 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0., 0., 0., 1., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0., 0., 0., 1., 0., 0., 0., 0.5]),\n",
    "    ],\n",
    "    'vtimes': [0,.25, .25, .5, .5, .75, .75, 1] # must be the same length as the number of break points in vsequence\n",
    "}\n",
    "\n",
    "###########################################################################\n",
    "#  And now some experimental ones ...\n",
    "###########################################################################\n",
    "\n",
    "morphs['pistons.applause.overlap'] = {\n",
    "    'vsequence': [\n",
    "        torch.tensor([1., 0., 0., 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([1., 0., 0., 0., 0., 0., 0., 0.5]),  \n",
    "        torch.tensor([1., 0., 1., 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([1., 0., 1., 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0., 0., 1., 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0., 0., 1., 0., 0., 0., 0., 0.5])\n",
    "    ],\n",
    "    'vtimes': [0,.2,.4,.6,.8, 1] # must be the same length as the number of break points in vsequence\n",
    "}\n",
    "\n",
    "morphs['bees.wind.overlap'] = {\n",
    "    'vsequence': [\n",
    "        torch.tensor([0., 0., 0., 1., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0., 0., 0., 1., 0., 0., 0., 0.5]),  \n",
    "        torch.tensor([0., 1., 0., 1., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0., 1., 0., 1., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0., 1., 0., 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0., 1., 0., 0., 0., 0., 0., 0.5])\n",
    "    ],\n",
    "    'vtimes': [0,.2,.4,.6,.8, 1] # must be the same length as the number of break points in vsequence\n",
    "}\n",
    "\n",
    "    \n",
    "morphs['pistons.and.applause.unison'] = {\n",
    "    # interesting .... the morph works better with the random context (at the beginning) that later when it freezes\n",
    "    'vsequence': [\n",
    "        torch.tensor([0.0, 0., 0.0, 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0.0, 0., 0.0, 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([1.0, 0., 1.0, 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([1.0, 0., 1.0, 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0.0, 0., 0.0, 0., 0., 0., 0., 0.5]),\n",
    "        torch.tensor([0.0, 0., 0.0, 0., 0., 0., 0., 0.5]),\n",
    "    ],\n",
    "    'vtimes': [0,.1, .4, .6, .9, 1] # must be the same length as the number of break points in vsequence\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters from yaml file and derive any necessary\n",
    "######################################################\n",
    "print(f\"will use paramfile= {paramfile}\") \n",
    "# Load YAML file\n",
    "with open(paramfile, 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "    \n",
    "inference_steps=86*gendur  #86 frames per second\n",
    "    \n",
    "TransformerClass =  globals().get(params['TransformerClass'])  \n",
    "print(f\"using TransformerClass = {params['TransformerClass']}\") \n",
    "print(f' and TransformerClass is class object {TransformerClass}')\n",
    "\n",
    "cond_size = 8 # num_classes + num params - not a FREE parameter!\n",
    "\n",
    "embed_size = params['tblock_input_size'] -cond_size # 240 #32  # embed_size must be divisible by num_heads and by num tokens\n",
    "print(f'embed_size is {embed_size}')\n",
    "\n",
    "\n",
    "fnamebase='out' + '.e' + str(params['tblock_input_size']-cond_size) + '.l' + str(params['num_layers']) + '.h' + str(params['num_heads']) + '_chkpt_' + str(cptnum).zfill(4) \n",
    "checkpoint_path = checkpoint_dir + '/' +  fnamebase  + '.pth' \n",
    "\n",
    "# for saving sound \n",
    "outdir=checkpoint_dir\n",
    "\n",
    "print(f'checkpoint_path = {checkpoint_path}, fnamebase = {fnamebase}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59747826",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEVICE == 'cuda' :\n",
    "    torch.cuda.device_count()\n",
    "    torch.cuda.get_device_properties(0).total_memory/1e9\n",
    "\n",
    "    device = torch.device(DEVICE) # if the docker was started with --gpus all, then can choose here with cuda:0 (or cpu)\n",
    "    torch.cuda.device_count()\n",
    "    print(f'memeory on cuda 0 is  {torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
    "else :\n",
    "    device=DEVICE\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344f167",
   "metadata": {},
   "source": [
    "# The inference method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbff75",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def inference(model, inference_cond, Ti_context_length, vocab_size, num_tokens, inference_steps, topn, fname) :\n",
    "    model.eval()\n",
    "    mask = generate_mask(Ti_context_length, Ti_context_length).to(device)\n",
    "\n",
    "    # The \"input data\" is random with a sequence length equal to the context length (and the mask) which is used \n",
    "    # to generate the first step of the output.It is not included in the output.\n",
    "    input_data = torch.randint(0, vocab_size, (1, Ti_context_length, num_tokens)).to(device)  # Smaller context window for inference\n",
    "    #Extend the first conditional vector to cover the \"input\" which is of length Ti_context_length\n",
    "    inference_cond = torch.cat([inference_cond[:, :1, :].repeat(1, Ti_context_length, 1), inference_cond], dim=1)\n",
    "    predictions = []\n",
    "\n",
    "    print(f' in the inference function, the shape of input_data is {input_data.shape} and the shape of the inference_cond is {inference_cond.shape}')\n",
    "\n",
    "    t0 = time.time()\n",
    "    for i in range(inference_steps):  # \n",
    "        if cond_size == 0:\n",
    "            output = model(input_data, None, mask) # step through \n",
    "        else : \n",
    "            output = model(input_data, inference_cond[:, i:Ti_context_length+i, :], mask) # step through\n",
    "\n",
    "        # This takes the last vector of the sequence (the new predicted token stack) so has size(b,steps,4,1024)\n",
    "        # This it takes the max across the last dimension (scores for each element of the vocabulary (for each of the 4 tokens))\n",
    "        # .max returns a duple of tensors, the first are the max vals (one for each token) and the second are the\n",
    "        #        indices in the range of the vocabulary size. \n",
    "        # THAT IS, the selected \"best\" tokens (one for each codebook) are taken independently\n",
    "        ########################### next_token = output[:, -1, :, :].max(-1)[1]  # Greedy decoding for simplicity\n",
    "        \n",
    "        next_token = sample_top_n(output[:, -1, :, :],topn) # topn=1 would be the same as max in the comment line above    \n",
    "        predictions.append(next_token)\n",
    "        input_data = torch.cat([input_data, next_token.unsqueeze(1)], dim=1)[:, 1:]  # Slide window\n",
    "\n",
    "    t1 = time.time()\n",
    "    inf_time = t1-t0\n",
    "    print(f'inference time for {inference_steps} steps, or {inference_steps/86} seconds of sound is {inf_time}' )\n",
    "\n",
    "    dacseq = torch.cat(predictions, dim=0).unsqueeze(0).transpose(1, 2)\n",
    "    if mask == None:\n",
    "        writeDACFile(fname + '_unmasked', dacseq)\n",
    "    else :\n",
    "        writeDACFile(fname, dacseq)   \n",
    "\n",
    "    print(f'dacseq shape written to file is of shape {dacseq.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the stored model\n",
    "\n",
    "print(f' About to call load_model with TransformerClass = {TransformerClass}')\n",
    "model, Ti_context_length, vocab_size, num_codebooks, cond_size = load_model(checkpoint_path,  TransformerClass, DEVICE)\n",
    "print(f'Mode loaded, context_length (Ti_context_length) = {Ti_context_length}')\n",
    "# Count the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {num_params}')\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e645e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the conditioning sequence from the breakpoint sequence, and plot the trajectories\n",
    "\n",
    "if cond_size == 0 :\n",
    "    inference_cond = None\n",
    "else : \n",
    "\n",
    "    inference_cond=interpolate_vectors(morphs[morphname]['vsequence'], [round(x * inference_steps) for x in morphs[morphname]['vtimes']]) #length must cover staring context window+inf steps\n",
    "\n",
    "\n",
    "    # Make a picture --------------------------------------------------------------\n",
    "    # Extract the 2D array of shape [n, m]\n",
    "    data = inference_cond[0]\n",
    "    # Find components that change over time\n",
    "    changing_indices = [i for i in range(cond_size) if not torch.all(data[:, i] == data[0, i])]\n",
    "\n",
    "    # Plot the changing components\n",
    "    plt.figure(figsize=(10, 3))\n",
    "\n",
    "    for i in changing_indices:\n",
    "        if i != 7 :\n",
    "            plt.plot(data[:, i], label=f'{int2classname[i]} ({i})')\n",
    "        else : \n",
    "            plt.plot(data[:, i], label=f'Parameter ({i})', linestyle='--')\n",
    "\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Component Values')\n",
    "    plt.title(f' {morphname}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    inference_cond=inference_cond.to(device)\n",
    "    print(f'shape of inf_cond is  = {inference_cond.shape}') \n",
    "\n",
    "\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9890f8",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d894ce",
   "metadata": {},
   "source": [
    "# Run the Transformer to generate the .dac file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfname=outdir+\"/\"+ \"dacs\" + \"/\" +  morphname + '_chkpt_' + str(cptnum).zfill(4) +  \"_steps_\"+str(inference_steps).zfill(4) +'.topn_'+ f\"{topn:04d}\"\n",
    "print(f'outfname is {outfname}')\n",
    "inference(model, inference_cond, Ti_context_length, vocab_size, num_codebooks, inference_steps, topn, outfname ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ebcb6",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe3681",
   "metadata": {},
   "source": [
    "# Decode the transformer-generated tokens to audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578957d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trained encodec from Descript\n",
    "# The first time you do this, it can take a while. Go get coffee. After that, it uses a cached version\n",
    "\n",
    "dacmodel_path = dac.utils.download(model_type=\"44khz\") \n",
    "print(f'The DAC decoder is in {dacmodel_path}')\n",
    "with torch.no_grad():\n",
    "    dacmodel = dac.DAC.load(dacmodel_path)\n",
    "\n",
    "    dacmodel.to(device); #wanna see the model? remove the semicolon\n",
    "    dacmodel.eval();  # need to be \"in eval mode\" in order to set the number of quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------  derived ------ don't change these \n",
    "selected_file=outfname + \".dac\"\n",
    "print(f' selected_file is {selected_file}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    dacfile = dac.DACFile.load(selected_file)\n",
    "    # FIRST - Decompress it back to an AudioSignal\\ from codes to z (1024) to signal   \n",
    "    print(f'dacfile.codes shape is: {dacfile.codes.shape}')\n",
    "    t0=time.time()\n",
    "    asig=dacmodel.decompress(dacfile)\n",
    "    t1=time.time()\n",
    "    \n",
    "    inf_time = t1-t0\n",
    "    print(f'decompress time for {asig.audio_data.shape[2]/44100} seconds of sound is {inf_time}' )\n",
    "    print(f'asig.audio_data.shape[2] is {asig.audio_data.shape[2]}')\n",
    "    \n",
    "    asig.cpu().widget()\n",
    "    asig.save_image(outfname + \".jpg\")\n",
    "    asig.audio_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94495345",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = asig.samples.view(-1).numpy()\n",
    "if SAVEWAV :  \n",
    "    sf.write(outfname + \".wav\", adata, 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Audio display\n",
    "plt.plot(adata)\n",
    "# Audio player\n",
    "ipd.Audio(adata, rate=44100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589869bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
